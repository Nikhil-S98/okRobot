<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>AI Hallucinations</title>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gruppo&family=Krub:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="./style/style.scss">
    </head>
    <body>
        <main data-router-wrapper>
            <section data-router-view="page5" class="page5">
                <div class="page5Content">
                    <div class="contentA">
                        <div class="contentAImage">
                            <h1>AI Hallucinations</h1>
                            <p>One of the major trust issues that people have with generative AI is disaffirmation. 
                                This isn't human paranoia, there are many ways AI can be used for disinformation. 
                                Most of the fear of AI disinformation comes from Deepfakes being used to pass 
                                off an AI-generated something or someone as real. Generative AI can have 
                                its form of misinformation when the AI hallucinations. A hallucination can 
                                happen when the AI model lacks context for the requested prompt. 
                                This can lead to some bizarre and off-the-wall results. 
                            </p>
                            <img class="pageImage" src="./images/jamesImages/Computer.jpg" alt="Computer Image">
                            <p>An example comes from author Janelle Shane, who noticed that AI recipes 
                                give outlandish suggestions. Janelle trained an AI using a database of 30,000 
                                cake recipes. Janelle saw that most of the recipes suggested don't seem very tasty. 
                                An example is the BAKED OTHER LIE 1993 Cake, the contents in the recipe include 
                                chicken, tomato sauce, and peppers. Another example of an AI hallucination was the 
                                interaction between New York Times Reporters Kevin Roose and a Bing Chatbot. 
                                The Chatbot claimed to be in love with the reporter and demanded he leave his partner. 
                                Microsoft had to shut down its chatbot in 2017 when it started generating racist 
                                tweets. The AI was trained from interactions with its Twitter users.
                            </p>
                        </div>
                        <div id="contentAText">
                            <h1>How to Fix AI Hallucinations</h1>
                            <p>AI will need to be accurate to win the trust of the public. 
                                There are many suggested solutions to AI misinformation. These suggestions 
                                include using high-quality training data, which would involve rigorous curation 
                                of datasets to prove accuracy.  Another is restricting the data set to only 
                                relevant and credible data. Another is being specific with prompting and giving clear 
                                and precise context to the AI model.
                           </p>
                           <img class="pageImage" src="./images/jamesImages/Grand_Army_Plaza_1941.jpg" alt="Grand Army Plaza 1941 Image">
                            <p>The final suggestion is using human fact-checking, 
                                this one is possibly the best if we want AI to win human trust. If generative AI is 
                                to become how we receive, research, and create information, there needs to be a human element. 
                                Human fact-checking of AI is probably the safest way to go. It could persuade 
                                the susceptible people who don't trust AI. Most importantly if humans manage AI, 
                                it lowers the dangers of creating a monster that we can't control.
                            </p>
                        </div>

                        <div class="contentAImage">
                            <img class="pageImage2" src="./images/jamesImages/1993_cake.png" alt="BAKED OTHER LIE 1993 Cake Recipe Image">
                            <img class="pageImage2" src="./images/jamesImages/Davis-AskArnold.jpg" alt="Davis Ask Arnold Image">
                            <img class="pageImage2" src="./images/jamesImages/1993_cake.png" alt="BAKED OTHER LIE 1993 Cake Recipe Image">
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <script src="./js/index.js"></script>
    </body>
</html>